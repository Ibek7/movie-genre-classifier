{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c108f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae80f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"../data/processed/cleaned_plots.csv\")\n",
    "\n",
    "# Recreate single-label target\n",
    "plots = df[\"Plot\"]\n",
    "genres = df[\"Genre\"].str.split(\"|\").apply(lambda g: g[0])\n",
    "\n",
    "# Stratify only if possible\n",
    "stratify_arg = genres if genres.value_counts().min() >= 2 else None\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    plots,\n",
    "    genres,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=stratify_arg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4911e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the artifacts you saved\n",
    "vec = joblib.load(\"../models/tfidf_vectorizer.joblib\")\n",
    "nb_model = joblib.load(\"../models/nb_model.joblib\")\n",
    "lr_model = joblib.load(\"../models/lr_model.joblib\")\n",
    "\n",
    "# Transform the test set\n",
    "X_te = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c0eaeec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m y_pred_nb = \u001b[43mnb_model\u001b[49m.predict(X_te)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m### Naive Bayes Performance ###\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAccuracy:\u001b[39m\u001b[33m\"\u001b[39m, accuracy_score(y_test, y_pred_nb))\n",
      "\u001b[31mNameError\u001b[39m: name 'nb_model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_nb = nb_model.predict(X_te)\n",
    "\n",
    "print(\"### Naive Bayes Performance ###\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9be284",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m y_pred_lr = \u001b[43mlr_model\u001b[49m.predict(X_te)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m### Logistic Regression Performance ###\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAccuracy:\u001b[39m\u001b[33m\"\u001b[39m, accuracy_score(y_test, y_pred_lr))\n",
      "\u001b[31mNameError\u001b[39m: name 'lr_model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_lr = lr_model.predict(X_te)\n",
    "\n",
    "print(\"### Logistic Regression Performance ###\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3caa59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_nb = confusion_matrix(y_test, y_pred_nb, labels=nb_model.classes_)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_nb, annot=True, fmt=\"d\",\n",
    "            xticklabels=nb_model.classes_,\n",
    "            yticklabels=nb_model.classes_)\n",
    "plt.title(\"Naive Bayes Confusion Matrix\")\n",
    "plt.ylabel(\"True Genre\")\n",
    "plt.xlabel(\"Predicted Genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_lr = confusion_matrix(y_test, y_pred_lr, labels=lr_model.classes_)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_lr, annot=True, fmt=\"d\",\n",
    "            xticklabels=lr_model.classes_,\n",
    "            yticklabels=lr_model.classes_)\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "plt.ylabel(\"True Genre\")\n",
    "plt.xlabel(\"Predicted Genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b027643",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_nb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m] = model_name\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df[[\u001b[33m'\u001b[39m\u001b[33mf1-score\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msupport\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m f1_nb = get_f1_df(y_test, \u001b[43my_pred_nb\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mNaive Bayes\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m f1_lr = get_f1_df(y_test, y_pred_lr, \u001b[33m'\u001b[39m\u001b[33mLogistic Regression\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Combine for easy comparison\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'y_pred_nb' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract and display top/bottom genres by F1-score for both models\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "def get_f1_df(y_true, y_pred, model_name):\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    df = pd.DataFrame(report).T\n",
    "    df = df[df.index.isin(y_true.unique())]  # Only keep actual genres\n",
    "    df['model'] = model_name\n",
    "    return df[['f1-score', 'precision', 'recall', 'support', 'model']]\n",
    "\n",
    "f1_nb = get_f1_df(y_test, y_pred_nb, 'Naive Bayes')\n",
    "f1_lr = get_f1_df(y_test, y_pred_lr, 'Logistic Regression')\n",
    "\n",
    "# Combine for easy comparison\n",
    "f1_all = pd.concat([f1_nb, f1_lr])\n",
    "\n",
    "# Show top 5 and bottom 5 genres by F1-score for each model\n",
    "for model in f1_all['model'].unique():\n",
    "    print(f\"\\nTop 5 genres by F1-score ({model}):\")\n",
    "    display(f1_all[f1_all['model'] == model].sort_values('f1-score', ascending=False).head(5))\n",
    "    print(f\"\\nBottom 5 genres by F1-score ({model}):\")\n",
    "    display(f1_all[f1_all['model'] == model].sort_values('f1-score', ascending=True).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fa7bf",
   "metadata": {},
   "source": [
    "## Model Evaluation Summary\n",
    "\n",
    "- **Overall Accuracy**: See above for both models.\n",
    "- **Top-performing genres**: The genres with the highest F1-scores are likely those with more samples and clearer language patterns.\n",
    "- **Genres to improve**: Genres with the lowest F1-scores may be underrepresented or have more ambiguous plot descriptions.\n",
    "- **Quick hypotheses**:\n",
    "    - Some genres (e.g., \"Horror\" or \"Documentary\") may underperform due to fewer samples or overlapping vocabulary with other genres.\n",
    "    - Consider consolidating rare genres or using multi-label classification if plots belong to multiple genres.\n",
    "    - Further balancing the dataset or aggregating similar genres could improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
